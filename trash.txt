# version: "3.8"

# services:
#   postgres:
#     image: postgres:13
#     container_name: postgres
#     restart: always
#     environment:
#       - POSTGRES_USER=airflow
#       - POSTGRES_PASSWORD=airflow
#       - POSTGRES_DB=airflow
#     volumes:
#       - postgres_data:/var/lib/postgresql/data
#     ports:
#       - "5432:5432"
#     networks:
#       - crypto-net

#   redis:
#     image: redis:latest
#     container_name: redis
#     ports:
#       - "6379:6379"
#     networks:
#       - crypto-net

#   airflow-init:
#     build: 
#       context: ./pipeline_orchestration
#       dockerfile: Dockerfile
#     environment:
#       - AIRFLOW__CORE__LOAD_EXAMPLES=False
#       - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
#     volumes:
#       - .airflow/apache/dags:/opt/airflow/dags
#       - .airflow/apache/logs:/opt/airflow/logs
#       - .airflow/apache/plugins:/opt/airflow/plugins
#     depends_on:
#       - postgres
#     networks:
#       - crypto-net
#     entrypoint: ["bash","-c","airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --email admin@example.com --role Admin"]
#   airflow-webserver:
#     build: 
#       context: ./pipeline_orchestration
#       dockerfile: Dockerfile  # Pointing to the Dockerfile where the custom image is built
#     container_name: airflow-webserver
#     restart: always
#     environment:
#       - LOAD_EXAMPLES=False
#       - AIRFLOW__CORE__EXECUTOR=LocalExecutor
#       - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
#       - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth  # Authentification de l'API
#       - AIRFLOW__WEBSERVER__RBAC=True  # Activer RBAC
#       - AIRFLOW__WEBSERVER__WORKERS=4  # Nombre de workers pour le serveur web
#     volumes:
#       # - ./apache/dags:/opt/airflow/dags
#       - ./apache/logs:/opt/airflow/logs
#       - ./apache/plugins:/opt/airflow/plugins
#       - /var/run/docker.sock
      

#     ports:
#       - "8080:8080"
#     depends_on:
#       - postgres
#       - airflow-init  
#     command: webserver
#     # entrypoint: ["bash","-c","airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --email admin@example.com --role Admin"]

#     networks:
#       - crypto-net

#   airflow-scheduler:
#     build: 
#       context: ./pipeline_orchestration
#       dockerfile: Dockerfile
#     container_name: airflow-scheduler
#     restart: always
#     environment:
#       - AIRFLOW__CORE__EXECUTOR=CeleryExecutor  # Use CeleryExecutor
#       - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
#       - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
#       - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
#       - AIRFLOW__CORE__LOAD_EXAMPLES=False
#     depends_on:
#       - airflow-webserver
#       - postgres
#       - airflow-init
#     volumes:
#       # - ./apache/dags:/opt/airflow/dags
#       - .airflow/apache/logs:/opt/airflow/logs
#       - .airflow/apache/plugins:/opt/airflow/plugins
#       - /var/run/docker.sock
#     command: scheduler
#     networks:
#       - crypto-net

#   airflow-worker:
#     build: 
#       context: ./pipeline_orchestration
#       dockerfile: Dockerfile
#     container_name: airflow-worker
#     restart: always
#     environment:
#       - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
#       - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
#       - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
#       - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
#       - AIRFLOW__CORE__LOAD_EXAMPLES=False
#     volumes:
#       # - ./apache/dags:/opt/airflow/dags
#       - .airflow/apache/logs:/opt/airflow/logs
#       - .airflow/apache/plugins:/opt/airflow/plugins
#       -  /var/run/docker.sock
#     depends_on:
#       - postgres
#       - airflow-init
#       - redis
#     command: celery worker
#     networks:
#       - crypto-net

#   data_ingestion:
#     build:
#       context: ./ingestion
#       dockerfile: Dockerfile
#     volumes:
#       - ./ingestion:/app
#     networks:
#       - crypto-net
#     command: tail -f /dev/null

#   # Hadoop NameNode
#   hadoop-namenode:
#     image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
#     container_name: namenode
#     environment:
#       - CLUSTER_NAME=test-cluster
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#       - HDFS_CONF_dfs_namenode_name_dir=file:///hadoop/dfs/name
#     ports:
#       - "9870:9870"
#     volumes:
#       - ./hadoop/namenode_data:/hadoop/dfs/name
#     networks:
#       - crypto-net


#   # Hadoop DataNode
#   hadoop-datanode:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode
#     environment:
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#       - HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data
#     ports:
#       - "9864:9864"
#     volumes:
#       - ./hadoop/datanode_data:/hadoop/dfs/data
#     networks:
#       - crypto-net
#     depends_on:
#       - hadoop-namenode


# volumes:
#   hdfs:
#   postgres_data:

# networks:
#   crypto-net:
#     driver: bridge


# -----------------------------------------------------------------------------------